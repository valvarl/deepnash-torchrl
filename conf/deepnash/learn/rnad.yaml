RNaDConfig:
  game_name: "example_game"
  trajectory_max: 15
  state_representation: "observation"
  policy_network_layers:
    - 128
    - 128
  batch_size: 128
  learning_rate: 0.001
  adam:
    b1: 0.9
    b2: 0.999
    eps: 1e-8
  clip_gradient: 5000
  target_network_avg: 0.005
  entropy_schedule_repeats:
    - 300
    - 2
  entropy_schedule_size:
    - 50
    - 150
  eta_reward_transform: 0.3
  nerd:
    beta: 2.5
    clip: 10000
  c_vtrace: 0.8
  seed: 1234