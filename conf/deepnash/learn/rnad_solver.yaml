defaults:
  - _self_
  - deployment_config: device_2_instance_1

adam_config:
  b1: 0.0
  b2: 0.999
  eps: 1e-8

nerd_config:
  beta: 2.0
  clip: 10000

rnad_config:
  game_name: "stratego"
  trajectory_max: 10
  state_representation: "info_set"  # or "observation"
  policy_network_layers: [256, 256]
  batch_size: 256
  learning_rate: 0.0005
  adam:
    b1: 0.0
    b2: 0.999
    eps: 1e-8
  clip_gradient: 10000
  target_network_avg: 0.001
  entropy_schedule_repeats: [200, 1]
  entropy_schedule_size: [100, 100]
  eta_reward_transform: 0.2
  nerd:
    beta: 2.0
    clip: 10000
  c_vtrace: 1.0
  seed: 42

collector_config:
  -

replay_buffer_config:
  -

deployment_config:
  mode: "parallel"  # or sequential (for single device only)
  sequential_actor_learner_ratio: 0.25
  sequential_step_duration: 300
  num_instances: 1
  instance_gpu_capacity: 2
  actor_gpu_count: 1
  learner_gpu_count: 1
  shared_replay_buffer: false

monitoring_config:
  wandb: true
  log_interval: 10
  expl_interval: 20
  checkpoint_interval: 50
  sync_intervals:
    intra_instance: 5
    inter_instance: 20
  actor_update_interval: 5

